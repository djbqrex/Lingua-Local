version: '3.8'

services:
  language-assistant:
    build:
      context: .
      dockerfile: backend/Dockerfile
      args:
        USE_GPU: "true"
    container_name: language-learning-assistant-gpu
    ports:
      - "8080:8000"
    volumes:
      - ./models:/app/models
      - ./backend:/app/backend
      - ./frontend:/app/frontend
      - ./tmp:/app/tmp
    environment:
      # Hardware - GPU enabled
      - DEVICE=cuda
      - COMPUTE_TYPE=float16
      
      # Models - can use larger models with GPU
      - STT_MODEL=medium
      - LLM_MODEL=qwen2.5-3b-instruct
      - TTS_VOICE=en_US-lessac-medium
      
      # Languages
      - SUPPORTED_LANGUAGES=en,es,fr,de,it,pt,ja,zh,ko,ar
      
      # Performance
      - MAX_AUDIO_LENGTH=30
      - STREAM_AUDIO=true
      - CACHE_MODELS=true
      
      # API
      - HOST=0.0.0.0
      - PORT=8000
      - CORS_ORIGINS=http://localhost:8080,http://127.0.0.1:8080
    restart: unless-stopped
    # GPU configuration
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 8G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    runtime: nvidia
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s

volumes:
  models:
    driver: local
